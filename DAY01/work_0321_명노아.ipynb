{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "train_dir = '../data/lang_data/train' # 학습 데이터가 있는 디렉토리\n",
    "save=[] # concatenate할 저장소 \n",
    "\n",
    "for i in range(len(os.listdir(train_dir))):    # train 폴더의 하위 문서를 하나씩 읽어들임\n",
    "    alpha='αabcdefghijklmnopqrstuvwxyz'   # 딕셔너리의 키\n",
    "    count=[0 for _ in range(27)]          # 딕셔너리의 값\n",
    "    alpha_dict=dict(zip(alpha, count))    # 딕셔너리 생성\n",
    "    with open(train_dir+\"/\"+os.listdir(train_dir)[i],\"r\") as f:\n",
    "        while True: # 한 글자씩 쪼개서, 알파벳 문자만 남기고 나머지는 제거\n",
    "            text = f.read(1)\n",
    "            if not text:\n",
    "                break # 끝에 다다르면 종료 \n",
    "            if text.lower() in alpha: # 알파벳을 소문자로 변환\n",
    "                text=text.lower()\n",
    "                alpha_dict[text]+=1 # 하나씩 나올때마다 값을 늘림\n",
    "    alpha_dict[\"α\"]=os.listdir(train_dir)[i][:2] # α : 딕셔너리에 언어 정보를 추가\n",
    "    if i==0:\n",
    "        save.append(list(alpha_dict.values())) # 첫 반복일 때, save가 0이므로, 그냥 저장\n",
    "    else: # 두 번째부터는 concatenate로 저장소에 추가\n",
    "        save=np.concatenate((np.array(save), [np.array(list(alpha_dict.values()))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "save=pd.DataFrame(save)\n",
    "target=save[save.columns[0]]\n",
    "feature=save[save.columns[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathn\\.conda\\envs\\Torch_PY38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le=LabelEncoder()\n",
    "target = le.fit_transform(target) # \"en\",\"fr\",\"id\",\"tl\"을 라벨 인코딩 => [0,1,2,3]\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature, target, test_size=0.2, random_state=42, stratify=target)\n",
    "\n",
    "model=LogisticRegression()\n",
    "model.fit(x_train,y_train)\n",
    "print(model.score(x_train,y_train))\n",
    "print(model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../data/lang_data/test' # 테스트 폴더 경로 지정 \n",
    "save=[]\n",
    "for i in range(len(os.listdir(train_dir))):\n",
    "    alpha='αabcdefghijklmnopqrstuvwxyz'\n",
    "    count=[0 for _ in range(27)]\n",
    "    alpha_dict=dict(zip(alpha, count))\n",
    "    with open(train_dir+\"/\"+os.listdir(train_dir)[i],\"r\") as f:\n",
    "        while True:\n",
    "            text = f.read(1)\n",
    "            if not text:\n",
    "                break\n",
    "            if text.lower() in alpha:\n",
    "                text=text.lower()\n",
    "                alpha_dict[text]+=1\n",
    "    alpha_dict[\"α\"]=os.listdir(train_dir)[i][:2]\n",
    "    if i==0:\n",
    "        save.append(list(alpha_dict.values()))\n",
    "    else:\n",
    "        save=np.concatenate((np.array(save), [np.array(list(alpha_dict.values()))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=pd.DataFrame(save)\n",
    "target=save[save.columns[0]]\n",
    "feature=save[save.columns[1:]]\n",
    "le=LabelEncoder()\n",
    "target = le.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(feature, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 34, 'b': 2, 'c': 14, 'd': 22, 'e': 45, 'f': 1, 'g': 5, 'h': 6, 'i': 31, 'j': 0, 'k': 0, 'l': 25, 'm': 13, 'n': 30, 'o': 20, 'p': 9, 'q': 2, 'r': 35, 's': 17, 't': 25, 'u': 16, 'v': 3, 'w': 0, 'x': 0, 'y': 2, 'z': 0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha='abcdefghijklmnopqrstuvwxyz'\n",
    "count=[0 for _ in range(26)]\n",
    "alpha_dict=dict(zip(alpha, count))\n",
    "\n",
    "txt = input(\"외국어를 입력해라!\") # Hello! My name is Myeong No ah, Yorosiku!\n",
    "for i in txt:\n",
    "    if i in alpha:\n",
    "        alpha_dict[i]+=1\n",
    "        \n",
    "        \n",
    "print(alpha_dict)\n",
    "model.predict([list(alpha_dict.values())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_PY38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
